import os
import time
import base64
import streamlit as st
import cv2
import numpy as np
import time

from skeleton_matcher import match_user_to_video
from video_composer import composite_user_with_video

import torch
import mediapipe as mp
import math
import argparse
from mediapipe import solutions as mp_solutions
from torch_geometric.data import Data
from torch_geometric.nn import global_mean_pool
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch.nn import Linear, Dropout

# â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
VIDEO = [
    #os.path.join(BASE_DIR, "assets", "video1.mp4"),
    #os.path.join(BASE_DIR, "assets", "video2.mp4"),
    #os.path.join(BASE_DIR, "assets", "video3.mp4")
    ['video/pokemon.mp4', 'pokemon', 0, 12, 20],
    'https://www.youtube.com/watch?v=9qyt9baCsQc',
    #'https://www.youtube.com/watch?v=9NdrlYmGbxA',
    'https://www.youtube.com/watch?v=col1BYgfMjs',
]

st.set_page_config(page_title="ğŸ•º ë™ì‘ ë”°ë¼í•˜ê¸° ì±Œë¦°ì§€", layout="wide")

st.markdown("""
<style>
  [data-testid="stVideo"] video,
  .stVideo > video {
    width: 100% !important;
    height: auto !important;
    aspect-ratio: 16 / 9;
    max-height: 65vh !important;
    object-fit: contain;
  }  
</style>
""", unsafe_allow_html=True)

# â”€â”€â”€ ìƒíƒœ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "step" not in st.session_state:
    st.session_state.step = -1  # ì‹œì‘í™”ë©´
if "final_image" not in st.session_state:
    st.session_state.final_image = None
if "success_triggered" not in st.session_state:
    st.session_state.success_triggered = False
if "show_countdown" not in st.session_state:
    st.session_state.show_countdown = True

# â”€â”€â”€ ì‹œì‘ í™”ë©´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def show_start_screen():
    st.markdown(
        "<h2 style='text-align: center; margin-top: 100px;'>ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ ê²Œì„ì„ ì‹œì‘í•˜ì„¸ìš”.</h2>",
        unsafe_allow_html=True
    )
    st.markdown("<div style='height: 250px;'></div>", unsafe_allow_html=True)

    button_css = """
        <style>
        .start-button {
            background: linear-gradient(90deg, #FF4B4B, #C62828);
            color: white;
            padding: 20px 60px;
            font-size: 24px;
            font-weight: bold;
            border: none;
            border-radius: 30px;
            box-shadow: 0 8px 16px rgba(0, 0, 0, 0.2);
            cursor: pointer;
            transition: transform 0.3s ease, background 0.4s ease;
        }
        .start-button:hover {
            transform: scale(1.2);
            background: linear-gradient(90deg, #42A5F5, #1E88E5);
        }
        </style>
    """
    st.markdown(button_css, unsafe_allow_html=True)

    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        if st.button("ğŸš€ ê²Œì„ ì‹œì‘", key="start"):
            st.session_state.step = 0
            st.session_state.show_countdown = True
            st.rerun()

############## GCN ëª¨ë¸ ì •ì˜ ##############
class GCN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, num_classes):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.lin = Linear(hidden_channels, num_classes)
        self.dropout = Dropout(p=0.3)

    def forward(self, x, edge_index, batch):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv2(x, edge_index)
        x = global_mean_pool(x, batch)
        return self.lin(x)

############## í•¨ìˆ˜: landmark -> graph ##############
def create_graph_data(pre_lm_flat):
    edges = [
        (0,1),(1,2),(2,3),(3,4),
        (5,6),(6,7),(7,8),(8,9),
        (10,11),(11,12),(12,13),(13,14),
        (11,12),(11,23),(12,24),
        (23,24),(23,25),(24,26),
        (25,27),(26,28),
        #(27,29),(28,30),
        #(29,31),(30,32)
    ]
    edges += [(j, i) for i, j in edges]
    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()

    x = torch.tensor(pre_lm_flat.reshape(-1, 4), dtype=torch.float)
    data = Data(x=x, edge_index=edge_index)
    data.batch = torch.zeros(x.size(0), dtype=torch.long)
    return data

############## ì„¤ì • ë° ì´ˆê¸°í™” ##############
threshold = 0.65 #0.67

class_names = ['Chair', 'Cobra', 'Dog', 'next_level','pokemon', 'Tree','umpa', 'Warrior']
n_landmarks = 33
torso_size_multiplier = 2.5
mp_pose = mp.solutions.pose
pose = mp_pose.Pose()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = GCN(in_channels=4, hidden_channels=64, num_classes=len(class_names)).to(device)
model.load_state_dict(torch.load('gcn_model.pth', map_location=device))
model.eval()

mp_drawing = mp_solutions.drawing_utils
mp_drawing_styles = mp_solutions.drawing_styles

############## í¬ì¦ˆ ì¶”ë¡  í•¨ìˆ˜ ##############
def process_frame(img):
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    result = pose.process(img_rgb)
    if not result.pose_landmarks:
        return img, 'No Pose'

    annotated_image = img.copy()
    lm_list = result.pose_landmarks.landmark

    # âœ… MediaPipe ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸°
    mp_drawing.draw_landmarks(
        annotated_image,
        result.pose_landmarks,
        mp_pose.POSE_CONNECTIONS,
        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()
    )

    # âœ… ì •ê·œí™” ë° ê·¸ë˜í”„ ì…ë ¥ ìƒì„±
    center_x = (lm_list[23].x + lm_list[24].x) * 0.5
    center_y = (lm_list[23].y + lm_list[24].y) * 0.5
    shoulders_x = (lm_list[11].x + lm_list[12].x) * 0.5
    shoulders_y = (lm_list[11].y + lm_list[12].y) * 0.5

    max_distance = max([
        math.sqrt((lm.x - center_x) ** 2 + (lm.y - center_y) ** 2)
        for lm in lm_list
    ])
    torso_size = math.sqrt((shoulders_x - center_x) ** 2 + (shoulders_y - center_y) ** 2)
    norm_factor = max(torso_size * torso_size_multiplier, max_distance)

    pre_lm_flat = np.array([
        [(lm.x - center_x) / norm_factor,
         (lm.y - center_y) / norm_factor,
         lm.z / norm_factor,
         lm.visibility]
        for lm in lm_list
    ]).flatten()

    graph = create_graph_data(pre_lm_flat).to(device)
    with torch.no_grad():
        out = model(graph.x, graph.edge_index, graph.batch)
        probs = F.softmax(out, dim=1).cpu().numpy()[0]
        conf = np.max(probs)
        pred_label = class_names[np.argmax(probs)] if conf > threshold else 'Unknown Pose'

    # âœ… ì˜ˆì¸¡ ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶”ê°€
    #cv2.putText(annotated_image, pred_label, (40, 80), cv2.FONT_HERSHEY_PLAIN, 6, (255, 0, 255), 4)
    return annotated_image, pred_label

# â”€â”€â”€ ì›¹ìº  í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_webcam_frame():
    webcam = cv2.VideoCapture(0)
    ret, frame = webcam.read()
    webcam.release()
    if not ret:
        return None
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    _, buffer = cv2.imencode('.jpg', frame)
    return base64.b64encode(buffer).decode()

# â”€â”€â”€ ìŠ¤í…Œì´ì§€ í™”ë©´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_stage(video):
    video_path = video[0]
    current = st.session_state.step
    total = len(VIDEO)

    st.markdown(f"<h1 style='text-align:center; font-size:4.5rem;'>ğŸ•º ìŠ¤í…Œì´ì§€ {current + 1} ğŸ‰</h1>", unsafe_allow_html=True)
    st.markdown("<p style='text-align:center; font-size:2.5rem;'>ğŸ‘€ ì˜ìƒì„ ë³´ê³  ë”°ë¼ í•´ ë³´ì„¸ìš”!</p>", unsafe_allow_html=True)

    # ì¹´ìš´íŠ¸ë‹¤ìš´
    if st.session_state.show_countdown:
        countdown = st.empty()
        for t in [3, 2, 1]:
            countdown.markdown(f"<h1 style='text-align:center'>{t}</h1>", unsafe_allow_html=True)
            time.sleep(1)
        countdown.markdown("<h1 style='text-align:center'>ì‹œì‘!</h1>", unsafe_allow_html=True)
        time.sleep(0.8)
        countdown.empty()
        st.session_state.show_countdown = False

    # ì˜ìƒ + ì›¹ìº 
    left, right = st.columns([1, 1])

    # ì™¼ìª½: ìœ íŠœë¸Œ ì˜ìƒ (YouTubeëŠ” ìë™ í¬ê¸° ì¡°ì •ë¨)
    with left:
        st.markdown("<h4 style='text-align:center;'>ğŸ“º ì˜ˆì œ ì˜ìƒ</h4>", unsafe_allow_html=True)
        st.video(video_path)

    # ì˜¤ë¥¸ìª½: ì‹¤ì‹œê°„ ì›¹ìº 
    with right:
        st.markdown("<h4 style='text-align:center;'>ğŸ“· ì‹¤ì‹œê°„ ì›¹ìº </h4>", unsafe_allow_html=True)
        stframe = st.empty()

        webcam = cv2.VideoCapture(0)  # ë˜ëŠ” ì™¸ë¶€ìº  ì¸ë±ìŠ¤ ì‚¬ìš©: VideoCapture(1)

        while True:
            ret_webcam, frame_webcam = webcam.read()
            if not ret_webcam or frame_webcam is None:
                st.warning("âš ï¸ ì›¹ìº  í”„ë ˆì„ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                break

            # âœ… process_frame í•¨ìˆ˜ë¡œ ìŠ¤ì¼ˆë ˆí†¤ ë° ì˜ˆì¸¡ ìˆ˜í–‰
            frame_out, pose_class = process_frame(frame_webcam)

            # âœ… ì¶œë ¥ í”„ë ˆì„ í¬ê¸° ì¡°ì • ë° í‘œì‹œ
            frame_out = cv2.resize(frame_out, (640, 360))
            stframe.image(frame_out, channels="BGR", use_container_width=True)

            time.sleep(1 / 30)  # ì•½ 30 FPS

        webcam.release()

    # ë™ì‘ í™•ì¸
    """
    if st.button("ğŸ¬ ë™ì‘ í™•ì¸í•˜ê¸°"):
        with st.spinner("ğŸ” ë¶„ì„ ì¤‘..."):
            matched = match_user_to_video(video_path)
            if matched:
                st.success("ğŸ¯ ë„ˆë¬´ ì˜í–ˆì–´ìš”!")
                st.balloons()
                time.sleep(1)
                #st.session_state.show_countdown = True
                st.rerun()
            else:
                st.error("ğŸ™… ë™ì‘ì´ ë‹¤ë¦…ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ë³´ì„¸ìš”!")
    """
    # ì´ì „/ë‹¤ìŒ
    col1, col2 = st.columns([1, 1])
    with col1:
        if current > 0:
            if st.button("âª ì´ì „ìœ¼ë¡œ"):
                st.session_state.step -= 1
                st.session_state.show_countdown = True
                st.rerun()
    with col2:
        if current < total:
            if st.button("â© ë‹¤ìŒìœ¼ë¡œ"):
                st.session_state.step += 1
                st.session_state.show_countdown = True
                st.rerun()

# â”€â”€â”€ ë§ˆì§€ë§‰ ê²°ê³¼ í™”ë©´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def show_final_stage():
    st.success("ğŸ‰ ëª¨ë“  ìŠ¤í…Œì´ì§€ ì™„ë£Œ!")
    img = composite_user_with_video()
    st.session_state.final_image = img
    st.image(img, caption="ê¸°ë…ì‚¬ì§„", use_container_width=True)

    img_bytes = cv2.imencode(".png", img)[1].tobytes()
    st.download_button("ğŸ“¥ ì‚¬ì§„ ë‹¤ìš´ë¡œë“œ", data=img_bytes, file_name="final_photo.png", mime="image/png")

# â”€â”€â”€ ë¼ìš°íŒ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.session_state.step == -1:
    show_start_screen()
elif st.session_state.step < len(VIDEO):
    run_stage(VIDEO[st.session_state.step])
else:
    show_final_stage()
